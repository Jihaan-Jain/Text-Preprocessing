{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1734c5f-eaa8-49c0-ade7-9ca4f55930f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9cbee76-248e-4c28-bc76-746801a59c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=requests.get('https://api.themoviedb.org/3/movie/top_rated?api_key=8265bd1679663a7ea12ac168da84d2e8&language=en-US&page=471')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6a75179-778d-48ad-a9d9-d0603ba2dfdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'adult': False,\n",
       "  'backdrop_path': '/bwIOaSrHGnzNBxs9aM3yKBlNKpp.jpg',\n",
       "  'genre_ids': [35],\n",
       "  'id': 419700,\n",
       "  'original_language': 'en',\n",
       "  'original_title': 'Sandy Wexler',\n",
       "  'overview': 'When a hapless but dedicated talent manager signs his first client who actually has talent, his career finally starts to take off.',\n",
       "  'popularity': 1.5808,\n",
       "  'poster_path': '/38SDqWOt4utiLOnWlas7BfrYQGS.jpg',\n",
       "  'release_date': '2017-04-14',\n",
       "  'title': 'Sandy Wexler',\n",
       "  'video': False,\n",
       "  'vote_average': 5.5,\n",
       "  'vote_count': 559},\n",
       " {'adult': False,\n",
       "  'backdrop_path': '/dWA4vVw6yyNdlHtSZhC1APlVDwT.jpg',\n",
       "  'genre_ids': [53, 28],\n",
       "  'id': 277355,\n",
       "  'original_language': 'en',\n",
       "  'original_title': 'Everly',\n",
       "  'overview': 'After she betrays a powerful mob boss, a woman matches wits and weaponry with a legion of killers who are out to collect the bounty on the heads of her and her family.',\n",
       "  'popularity': 2.331,\n",
       "  'poster_path': '/zYopOcb4VQQpcyJBtceyWKwYXsy.jpg',\n",
       "  'release_date': '2014-09-20',\n",
       "  'title': 'Everly',\n",
       "  'video': False,\n",
       "  'vote_average': 5.493,\n",
       "  'vote_count': 600},\n",
       " {'adult': False,\n",
       "  'backdrop_path': '/srHiCBm91irinD9ktbVGtsJT0dT.jpg',\n",
       "  'genre_ids': [28, 12, 35, 10751],\n",
       "  'id': 11335,\n",
       "  'original_language': 'en',\n",
       "  'original_title': 'Popeye',\n",
       "  'overview': \"Popeye is a super-strong, spinach-scarfing sailor man who's searching for his father. During a storm that wrecks his ship, Popeye washes ashore and winds up rooming at the Oyl household, where he meets Olive. Before he can win her heart, he must first contend with Olive's fiancé, Bluto.\",\n",
       "  'popularity': 2.3912,\n",
       "  'poster_path': '/hCCp4pESgYY6uTFARYf4UYV1BdU.jpg',\n",
       "  'release_date': '1980-12-12',\n",
       "  'title': 'Popeye',\n",
       "  'video': False,\n",
       "  'vote_average': 5.493,\n",
       "  'vote_count': 643},\n",
       " {'adult': False,\n",
       "  'backdrop_path': '/zJqv6s4XCdHWrnoqfLqlGzzxSjH.jpg',\n",
       "  'genre_ids': [53, 878, 9648],\n",
       "  'id': 437,\n",
       "  'original_language': 'en',\n",
       "  'original_title': 'Cube 2: Hypercube',\n",
       "  'overview': 'Eight strangers awaken with no memory, in a puzzling cube-shaped room where the laws of physics do not always apply.',\n",
       "  'popularity': 1.7191,\n",
       "  'poster_path': '/bEqqwtwUP7lm56VyeVONhv9JtYu.jpg',\n",
       "  'release_date': '2002-04-15',\n",
       "  'title': 'Cube 2: Hypercube',\n",
       "  'video': False,\n",
       "  'vote_average': 5.492,\n",
       "  'vote_count': 1492},\n",
       " {'adult': False,\n",
       "  'backdrop_path': '/9oLrsekWm3sfEGw5HOav6roW1h7.jpg',\n",
       "  'genre_ids': [28, 12, 35, 53],\n",
       "  'id': 3132,\n",
       "  'original_language': 'en',\n",
       "  'original_title': 'Bad Company',\n",
       "  'overview': 'When a Harvard-educated CIA agent is killed during an operation, the secret agency recruits his twin brother.',\n",
       "  'popularity': 2.0338,\n",
       "  'poster_path': '/umu141mcfIEhRLgyQp7TWlGJFW.jpg',\n",
       "  'release_date': '2002-06-07',\n",
       "  'title': 'Bad Company',\n",
       "  'video': False,\n",
       "  'vote_average': 5.491,\n",
       "  'vote_count': 796},\n",
       " {'adult': False,\n",
       "  'backdrop_path': '/2gccYK4lqVeRzrXYZmiZhL0P7Ft.jpg',\n",
       "  'genre_ids': [28, 12, 53],\n",
       "  'id': 230179,\n",
       "  'original_language': 'en',\n",
       "  'original_title': 'Big Game',\n",
       "  'overview': 'Air Force One is shot down by terrorists, leaving the President of the United States stranded in the wilderness of Finland. 13-year-old Oskari is on a hunting mission to prove his maturity to his kinsfolk by tracking down a deer, but instead discovers the President in an escape pod. With the terrorists closing in to capture their prize, the unlikely duo team up to escape their hunters.',\n",
       "  'popularity': 2.3047,\n",
       "  'poster_path': '/iZZ5IikYOp3VlyrzyJs4hxSHHXi.jpg',\n",
       "  'release_date': '2015-03-19',\n",
       "  'title': 'Big Game',\n",
       "  'video': False,\n",
       "  'vote_average': 5.489,\n",
       "  'vote_count': 1168},\n",
       " {'adult': False,\n",
       "  'backdrop_path': '/750Uayj0Bww4QfqMUJCymW3EJkC.jpg',\n",
       "  'genre_ids': [12, 10751],\n",
       "  'id': 36355,\n",
       "  'original_language': 'en',\n",
       "  'original_title': 'Flipper',\n",
       "  'overview': 'Sandy Ricks is sent by his mom to Coral Key, a rustic island in the Florida keys, to spend the summer with his uncle Porter Ricks. Sandy dislikes everything about his new environment until a new friend comes into his life, a dolphin named Flipper, that brings uncle and nephew together and leads Sandy on the summer adventure of a lifetime.',\n",
       "  'popularity': 1.5493,\n",
       "  'poster_path': '/Aivc5cbCTLcTOjHlOwOQux0WBcV.jpg',\n",
       "  'release_date': '1996-05-17',\n",
       "  'title': 'Flipper',\n",
       "  'video': False,\n",
       "  'vote_average': 5.489,\n",
       "  'vote_count': 321},\n",
       " {'adult': False,\n",
       "  'backdrop_path': '/pSNwe7G9bESwlCBVurlwRs7fMZU.jpg',\n",
       "  'genre_ids': [35],\n",
       "  'id': 4257,\n",
       "  'original_language': 'en',\n",
       "  'original_title': 'Scary Movie 4',\n",
       "  'overview': 'Cindy finds out the house she lives in is haunted by a little boy and goes on a quest to find out who killed him and why. Also, Alien \"Tr-iPods\" are invading the world and she has to uncover the secret in order to stop them.',\n",
       "  'popularity': 6.2988,\n",
       "  'poster_path': '/sEqFdw1wLtY94RKCSPolsHWzn6r.jpg',\n",
       "  'release_date': '2006-04-12',\n",
       "  'title': 'Scary Movie 4',\n",
       "  'video': False,\n",
       "  'vote_average': 5.5,\n",
       "  'vote_count': 3335},\n",
       " {'adult': False,\n",
       "  'backdrop_path': '/jyf7bNJs4CpZMwoeyo3RbbVtA9r.jpg',\n",
       "  'genre_ids': [18, 10749],\n",
       "  'id': 444902,\n",
       "  'original_language': 'en',\n",
       "  'original_title': 'Dirty Dancing',\n",
       "  'overview': 'Spending the summer at a Catskills resort with her family, Frances \"Baby\" Houseman falls in love with the camp\\'s dance instructor, Johnny Castle, and nothing is ever the same for anyone in the Houseman family.',\n",
       "  'popularity': 1.2346,\n",
       "  'poster_path': '/ySL7B4FgYZR5xFBWc7c7lssuCN9.jpg',\n",
       "  'release_date': '2017-05-24',\n",
       "  'title': 'Dirty Dancing',\n",
       "  'video': False,\n",
       "  'vote_average': 5.489,\n",
       "  'vote_count': 436},\n",
       " {'adult': False,\n",
       "  'backdrop_path': '/gFsBGyMJezO1wMRmEjBX8YEh0Mc.jpg',\n",
       "  'genre_ids': [35],\n",
       "  'id': 12121,\n",
       "  'original_language': 'en',\n",
       "  'original_title': 'Made in America',\n",
       "  'overview': \"A young black woman discovers that her father was a sperm donor, and if that wasn't bad enough, he's white.\",\n",
       "  'popularity': 1.0559,\n",
       "  'poster_path': '/2frB553lKEm1q4dds5PUqLNQA3z.jpg',\n",
       "  'release_date': '1993-01-06',\n",
       "  'title': 'Made in America',\n",
       "  'video': False,\n",
       "  'vote_average': 5.486,\n",
       "  'vote_count': 367},\n",
       " {'adult': False,\n",
       "  'backdrop_path': '/rrucFNW1qOgSPL4n2Fy6CdpBDEY.jpg',\n",
       "  'genre_ids': [27, 35],\n",
       "  'id': 1161048,\n",
       "  'original_language': 'sv',\n",
       "  'original_title': 'Konferensen',\n",
       "  'overview': 'A ragtag group of public sector employees battle not only their own discord but also a bloodthirsty killer during a seemingly innocuous retreat.',\n",
       "  'popularity': 1.6729,\n",
       "  'poster_path': '/ySjgDgVJrHBS2Iil7TKH1KxVfAC.jpg',\n",
       "  'release_date': '2023-10-13',\n",
       "  'title': 'The Conference',\n",
       "  'video': False,\n",
       "  'vote_average': 5.485,\n",
       "  'vote_count': 360},\n",
       " {'adult': False,\n",
       "  'backdrop_path': '/6QH3JB36yQJ5JkBlIVJE0P0pJyG.jpg',\n",
       "  'genre_ids': [53, 80, 28],\n",
       "  'id': 262338,\n",
       "  'original_language': 'en',\n",
       "  'original_title': 'Good People',\n",
       "  'overview': \"Tom and Anna Wright, a young American couple, fall into severe debt while renovating Tom's family home in London. As the couple faces the loss of their dream to have a house and start a family, they discover that the tenant in the apartment below them is dead, and he's left behind a stash of cash—$400,000 worth. Though initially hesitant, Tom and Anna decide that the plan is simple: all they have to do is quietly take the money and use only what's necessary to get them out of debt. But when they start spending the money and can't seem to stop, they find themselves the target of a deadly adversary—the thief who stole it—and that's when very bad things start happening to good people.\",\n",
       "  'popularity': 2.0963,\n",
       "  'poster_path': '/lHTPJGxQtYDCXlMhkHoxhKfOeqC.jpg',\n",
       "  'release_date': '2014-08-12',\n",
       "  'title': 'Good People',\n",
       "  'video': False,\n",
       "  'vote_average': 5.485,\n",
       "  'vote_count': 471},\n",
       " {'adult': False,\n",
       "  'backdrop_path': '/jBxQjghhOR3yyhf95cZtwqJ3Van.jpg',\n",
       "  'genre_ids': [35, 80],\n",
       "  'id': 97434,\n",
       "  'original_language': 'en',\n",
       "  'original_title': 'Gambit',\n",
       "  'overview': 'An art curator decides to seek revenge on his abusive boss by conning him into buying a fake Monet, but his plan requires the help of an eccentric and unpredictable Texas rodeo queen.',\n",
       "  'popularity': 1.6815,\n",
       "  'poster_path': '/6KOHKBa6toWdGOKHkzmLGjlOLdS.jpg',\n",
       "  'release_date': '2012-04-25',\n",
       "  'title': 'Gambit',\n",
       "  'video': False,\n",
       "  'vote_average': 5.485,\n",
       "  'vote_count': 687},\n",
       " {'adult': False,\n",
       "  'backdrop_path': '/mNINSJHjxQ7D1dDsV7gZg1exVST.jpg',\n",
       "  'genre_ids': [35],\n",
       "  'id': 86868,\n",
       "  'original_language': 'fr',\n",
       "  'original_title': 'La Vérité si je mens ! 3',\n",
       "  'overview': 'Eddie, Dov, Yvan and the others ... Our warm friends have migrated from the moribund Sentier to the flourishing suburb of Aubervilliers ... Where the old Jewish entrepreneurs left the ground to young courageous and dynamic Chinese wholesalers ...  The little band is as close to each other as in previous episodes, and life goes on at the mercy of small family events and business.  Dov still seems frivolous, enterprising Eddie, chilled Yvan, casual Karine, resolute Sandra, naive Chochana, irresponsible Serge and mythomaniac. As for Patrick, he is in love and the happy elected is far from easy to access.  Everything would be fine until a bad wind brings its share of adversity seriously compromising the cohesion of the group.  Will they succumb under the storm to the turmoil, or, once again, by mutual aid, cunning and skill, will they triumph over the crisis with panache?',\n",
       "  'popularity': 0.851,\n",
       "  'poster_path': '/i2U9VV2WWBXeA3Zc0jQFfYqnqB0.jpg',\n",
       "  'release_date': '2012-01-31',\n",
       "  'title': 'Would I Lie to You? 3',\n",
       "  'video': False,\n",
       "  'vote_average': 5.485,\n",
       "  'vote_count': 338},\n",
       " {'adult': False,\n",
       "  'backdrop_path': '/i64i2v5EvY0qmpUkyKs5cSOwdGG.jpg',\n",
       "  'genre_ids': [35, 27, 53],\n",
       "  'id': 10342,\n",
       "  'original_language': 'en',\n",
       "  'original_title': 'Teaching Mrs. Tingle',\n",
       "  'overview': \"A bright high-school senior has her impending status as valedictorian jeopardized when her bitter history teacher, Mrs. Tingle, gives her a poor grade on a project.  When an attempt to get ahead in Mrs. Tingle's class goes awry, mayhem ensues and friendships, loyalties and trust are tested by the teacher's intricate mind-games.\",\n",
       "  'popularity': 1.7177,\n",
       "  'poster_path': '/cGv0tkWZZSmoaPYiOsDcGmMhKq7.jpg',\n",
       "  'release_date': '1999-08-11',\n",
       "  'title': 'Teaching Mrs. Tingle',\n",
       "  'video': False,\n",
       "  'vote_average': 5.485,\n",
       "  'vote_count': 342},\n",
       " {'adult': False,\n",
       "  'backdrop_path': '/9Lm94WVtV4HPxr7NWNlwkhLKSeH.jpg',\n",
       "  'genre_ids': [12, 14, 28],\n",
       "  'id': 23047,\n",
       "  'original_language': 'en',\n",
       "  'original_title': 'Season of the Witch',\n",
       "  'overview': 'A 14th century Crusader returns with his comrade to a homeland devastated by the Black Plague. The Church commands the two knights to transport a witch to a remote abbey, where monks will perform a ritual in hopes of ending the pestilence.',\n",
       "  'popularity': 3.6912,\n",
       "  'poster_path': '/dNVqi8EsBtnnwfR6TgiWM51JOuS.jpg',\n",
       "  'release_date': '2011-01-07',\n",
       "  'title': 'Season of the Witch',\n",
       "  'video': False,\n",
       "  'vote_average': 5.484,\n",
       "  'vote_count': 2465},\n",
       " {'adult': False,\n",
       "  'backdrop_path': '/jdmRey22HOSyfL44PbYYImZUk3L.jpg',\n",
       "  'genre_ids': [12, 28, 18, 14],\n",
       "  'id': 7840,\n",
       "  'original_language': 'en',\n",
       "  'original_title': '10,000 BC',\n",
       "  'overview': \"A prehistoric epic that follows a young mammoth hunter's journey through uncharted territory to secure the future of his tribe.\",\n",
       "  'popularity': 3.3838,\n",
       "  'poster_path': '/9I7gV6wRbGnbfI3XOKjHeLMjYEo.jpg',\n",
       "  'release_date': '2008-03-04',\n",
       "  'title': '10,000 BC',\n",
       "  'video': False,\n",
       "  'vote_average': 5.483,\n",
       "  'vote_count': 3006},\n",
       " {'adult': False,\n",
       "  'backdrop_path': '/j4eGK1WH9YZWoDNDM9mAsF3mhLx.jpg',\n",
       "  'genre_ids': [28, 80, 53],\n",
       "  'id': 399173,\n",
       "  'original_language': 'en',\n",
       "  'original_title': 'The Assignment',\n",
       "  'overview': 'Ace assassin Frank Kitchen is double crossed by gangsters and falls into the hands of rogue surgeon known as The Doctor who turns him into a woman. The hitman, now a hitwoman, sets out for revenge, aided by a nurse named Johnnie who also has secrets.',\n",
       "  'popularity': 2.4762,\n",
       "  'poster_path': '/3wx83iGCATiIAyrZHI6AeeNfiAT.jpg',\n",
       "  'release_date': '2016-06-08',\n",
       "  'title': 'The Assignment',\n",
       "  'video': False,\n",
       "  'vote_average': 5.483,\n",
       "  'vote_count': 524},\n",
       " {'adult': False,\n",
       "  'backdrop_path': '/2zg3nm55gMUi5lFzCdXMjnYJjS9.jpg',\n",
       "  'genre_ids': [28, 12, 27],\n",
       "  'id': 11237,\n",
       "  'original_language': 'en',\n",
       "  'original_title': 'Anacondas: The Hunt for the Blood Orchid',\n",
       "  'overview': 'A scientific expedition sets out for Borneo to seek a flower called the Blood Orchid, which could grant extended life. Meanwhile, they run afoul of snakes and each other.',\n",
       "  'popularity': 6.2269,\n",
       "  'poster_path': '/ceMVASYCqV8dcdh20AcWf0lrJ8g.jpg',\n",
       "  'release_date': '2004-08-25',\n",
       "  'title': 'Anacondas: The Hunt for the Blood Orchid',\n",
       "  'video': False,\n",
       "  'vote_average': 5.5,\n",
       "  'vote_count': 1008},\n",
       " {'adult': False,\n",
       "  'backdrop_path': '/txfEqZ0eT5tcPcWnj1XJ6LLKoAZ.jpg',\n",
       "  'genre_ids': [35],\n",
       "  'id': 513083,\n",
       "  'original_language': 'en',\n",
       "  'original_title': 'Wine Country',\n",
       "  'overview': 'A group of friends head to the land of oaky Chardonnays and big, bold Cabernet Sauvignons for one member of the squad’s 50th birthday party.',\n",
       "  'popularity': 1.2459,\n",
       "  'poster_path': '/mi5l29DQFdwrTXxbqM2OLbs830H.jpg',\n",
       "  'release_date': '2019-05-08',\n",
       "  'title': 'Wine Country',\n",
       "  'video': False,\n",
       "  'vote_average': 5.482,\n",
       "  'vote_count': 511}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.json()['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8ed6a355-6310-4cea-b7d6-7e2595b837b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(a.json()['results'])[['overview']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be8c945e-7520-47d3-a69b-9d0617e1793e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When a hapless but dedicated talent manager si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>After she betrays a powerful mob boss, a woman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Popeye is a super-strong, spinach-scarfing sai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eight strangers awaken with no memory, in a pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When a Harvard-educated CIA agent is killed du...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            overview\n",
       "0  When a hapless but dedicated talent manager si...\n",
       "1  After she betrays a powerful mob boss, a woman...\n",
       "2  Popeye is a super-strong, spinach-scarfing sai...\n",
       "3  Eight strangers awaken with no memory, in a pu...\n",
       "4  When a Harvard-educated CIA agent is killed du..."
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c1e629b2-994c-473e-9b08-0253162eeeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eight strangers awaken with no memory, in a puzzling cube-shaped room where the laws of physics do not always apply.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overview'][3].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "631d8ac2-60d3-4827-8244-1da8a37d0037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['overview']=df['overview'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d4becdde-2df7-40ef-ad86-15d99e90cd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when a hapless but dedicated talent manager si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>after she betrays a powerful mob boss, a woman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>popeye is a super-strong, spinach-scarfing sai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eight strangers awaken with no memory, in a pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when a harvard-educated cia agent is killed du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>air force one is shot down by terrorists, leav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sandy ricks is sent by his mom to coral key, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cindy finds out the house she lives in is haun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spending the summer at a catskills resort with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a young black woman discovers that her father ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a ragtag group of public sector employees batt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tom and anna wright, a young american couple, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>an art curator decides to seek revenge on his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>eddie, dov, yvan and the others ... our warm f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>a bright high-school senior has her impending ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>a 14th century crusader returns with his comra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>a prehistoric epic that follows a young mammot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ace assassin frank kitchen is double crossed b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>a scientific expedition sets out for borneo to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>a group of friends head to the land of oaky ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             overview\n",
       "0   when a hapless but dedicated talent manager si...\n",
       "1   after she betrays a powerful mob boss, a woman...\n",
       "2   popeye is a super-strong, spinach-scarfing sai...\n",
       "3   eight strangers awaken with no memory, in a pu...\n",
       "4   when a harvard-educated cia agent is killed du...\n",
       "5   air force one is shot down by terrorists, leav...\n",
       "6   sandy ricks is sent by his mom to coral key, a...\n",
       "7   cindy finds out the house she lives in is haun...\n",
       "8   spending the summer at a catskills resort with...\n",
       "9   a young black woman discovers that her father ...\n",
       "10  a ragtag group of public sector employees batt...\n",
       "11  tom and anna wright, a young american couple, ...\n",
       "12  an art curator decides to seek revenge on his ...\n",
       "13  eddie, dov, yvan and the others ... our warm f...\n",
       "14  a bright high-school senior has her impending ...\n",
       "15  a 14th century crusader returns with his comra...\n",
       "16  a prehistoric epic that follows a young mammot...\n",
       "17  ace assassin frank kitchen is double crossed b...\n",
       "18  a scientific expedition sets out for borneo to...\n",
       "19  a group of friends head to the land of oaky ch..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "03d19945-7b7b-4389-bdd6-735096f2de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # Importing the regular expressions module\n",
    "\n",
    "# Function to remove HTML tags from a given text\n",
    "def remove_html_tags(text):\n",
    "    # This pattern matches anything that looks like an HTML tag (e.g., <p>, <div>, </b>, etc.)\n",
    "    pattern = re.compile('<.*?>')\n",
    "    \n",
    "    # Substitute all HTML tags with an empty string '' (i.e., remove them)\n",
    "    return pattern.sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6ffbd259-1d2c-439f-b8fb-4327f37ee1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Movie 1 Actor - Aamir Khan Click here to download'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"<html><body><p> Movie 1</p><p> Actor - Aamir Khan</p><p> Click here to <a href='http://google.com'>download</a></p></body></html>\"\n",
    "remove_html_tags(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a6cc78-6dd0-4bf6-9fe7-709782061cc3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7f467edb-f97b-48cc-aa78-0b54a8895a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['overview']=df['overview'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9e7997b3-6139-4029-ae1e-0580facf6a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'air force one is shot down by terrorists, leaving the president of the united states stranded in the wilderness of finland. 13-year-old oskari is on a hunting mission to prove his maturity to his kinsfolk by tracking down a deer, but instead discovers the president in an escape pod. with the terrorists closing in to capture their prize, the unlikely duo team up to escape their hunters.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overview'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "165ceb6f-2df5-41da-be7d-ff91e69f53da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5f85ec23-6854-446c-92c4-b5cc0dfd1043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check out my notebook '"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'Check out my notebook https://www.kaggle.com/campusx/notebook8223fc1abb'\n",
    "text2 = 'Check out my notebook http://www.kaggle.com/campusx/notebook8223fc1abb'\n",
    "text3 = 'Google search here www.google.com'\n",
    "text4 = 'For notebook click https://www.kaggle.com/campusx/notebook8223fc1abb to search check www.google.com'\n",
    "remove_url(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c11cd5f8-c099-4faf-8951-5e061305246d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string,time\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "69135c6f-4ee9-4711-9175-f0dcad26f828",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "87d6412e-80fc-4bca-b650-bb1b335a1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text=text.replace(char,'')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ae2cb99e-99b6-41fd-96d2-39f336f20ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string With Punctuation\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "text = 'string. With. Punctuation?'\n",
    "# Importing the time module to measure execution time\n",
    "import time\n",
    "\n",
    "# Start measuring time before the function call\n",
    "start = time.time()\n",
    "\n",
    "# Call the remove_punc function and print the result\n",
    "# (Assumes 'text' is a variable holding your input string and remove_punc is a defined function)\n",
    "print(remove_punc(text))\n",
    "\n",
    "# Calculate how much time has passed since the function started\n",
    "time1 = time.time() - start\n",
    "\n",
    "# Multiply the time taken for one execution by 50,000 to estimate total time for 50,000 runs\n",
    "print(time1 * 50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fa2f4b72-b7c5-43e1-b3c3-76d94b8fa0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('', '', exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "13f3fe0e-dd6e-4fae-9874-7e5619bd9b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "remove_punc1(text)\n",
    "time2 = time.time() - start\n",
    "print(time2*50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7fc7c5c0-0d3b-438e-b471-4828a509c362",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m time1\u001b[38;5;241m/\u001b[39mtime2\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "time1/time2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3ba56749-ba90-4ff9-b200-8062635e10bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'air force one is shot down by terrorists, leaving the president of the united states stranded in the wilderness of finland. 13-year-old oskari is on a hunting mission to prove his maturity to his kinsfolk by tracking down a deer, but instead discovers the president in an escape pod. with the terrorists closing in to capture their prize, the unlikely duo team up to escape their hunters.'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overview'][5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "44ef37d6-c6cd-4fb8-8eb4-b306e8c8a9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'air force one is shot down by terrorists leaving the president of the united states stranded in the wilderness of finland 13yearold oskari is on a hunting mission to prove his maturity to his kinsfolk by tracking down a deer but instead discovers the president in an escape pod with the terrorists closing in to capture their prize the unlikely duo team up to escape their hunters'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punc1(df['overview'][5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8fa1c4f7-45c3-4992-b0d2-733e363c508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = {\n",
    "    \"LOL\": \"laughing out loud\",\n",
    "    \"OMG\": \"oh my god\",\n",
    "    \"IDK\": \"I don't know\",\n",
    "    \"BRB\": \"be right back\",\n",
    "    \"IMHO\": \"in my humble opinion\",\n",
    "    \"TTYL\": \"talk to you later\",\n",
    "    \"BTW\": \"by the way\",\n",
    "    \"TBH\": \"to be honest\",\n",
    "    \"FYI\": \"for your information\",\n",
    "    \"GTG\": \"got to go\",\n",
    "    \"TTYS\": \"talk to you soon\",\n",
    "    \"LMAO\": \"laughing my ass off\",\n",
    "    \"ROFL\": \"rolling on the floor laughing\",\n",
    "    \"SMH\": \"shaking my head\",\n",
    "    \"BFF\": \"best friends forever\",\n",
    "    \"JK\": \"just kidding\",\n",
    "    \"IDC\": \"I don't care\",\n",
    "    \"ILY\": \"I love you\",\n",
    "    \"YOLO\": \"you only live once\",\n",
    "    \"NVM\": \"never mind\",\n",
    "    \"OMW\": \"on my way\",\n",
    "    \"IRL\": \"in real life\",\n",
    "    \"TMI\": \"too much information\",\n",
    "    \"FOMO\": \"fear of missing out\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "faa8d50e-400c-464b-a06e-eb2b1b6d1969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text=[]\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0709934f-dfb2-4a86-a6a4-80ff2fe79d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in my humble opinion he is the best'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion('IMHO he is the best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "884a9c3c-0bc7-4e10-ae9e-44275ff913b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for your information delhi is the capital of india'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion('FYI delhi is the capital of india')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "efa47187-7ade-499b-8441-b1b31959bc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Using cached textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting nltk>=3.9 (from textblob)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Using cached textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: nltk, textblob\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.8.1\n",
      "    Uninstalling nltk-3.8.1:\n",
      "      Successfully uninstalled nltk-3.8.1\n",
      "Successfully installed nltk-3.9.1 textblob-0.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\typing_extensions-4.11.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\typing_extensions-4.11.0.dist-info due to invalid metadata entry 'name'\n",
      "    WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "    WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "01b34804-aa0b-4737-bf24-aeb1397a6ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'certain conditions during several generations are modified in the same manner.'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_text = 'ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.'\n",
    "textBlb=TextBlob(incorrect_text)\n",
    "textBlb.correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0fa5da93-5e2a-434a-a465-b0c49970f903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jainj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1d13ffd1-9385-48fa-a9a8-51b7edeae11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0dd63abd-d08a-47a2-89a9-c92e92d892d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'la',\n",
       " 'que',\n",
       " 'el',\n",
       " 'en',\n",
       " 'y',\n",
       " 'a',\n",
       " 'los',\n",
       " 'del',\n",
       " 'se',\n",
       " 'las',\n",
       " 'por',\n",
       " 'un',\n",
       " 'para',\n",
       " 'con',\n",
       " 'no',\n",
       " 'una',\n",
       " 'su',\n",
       " 'al',\n",
       " 'lo',\n",
       " 'como',\n",
       " 'más',\n",
       " 'pero',\n",
       " 'sus',\n",
       " 'le',\n",
       " 'ya',\n",
       " 'o',\n",
       " 'este',\n",
       " 'sí',\n",
       " 'porque',\n",
       " 'esta',\n",
       " 'entre',\n",
       " 'cuando',\n",
       " 'muy',\n",
       " 'sin',\n",
       " 'sobre',\n",
       " 'también',\n",
       " 'me',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'donde',\n",
       " 'quien',\n",
       " 'desde',\n",
       " 'todo',\n",
       " 'nos',\n",
       " 'durante',\n",
       " 'todos',\n",
       " 'uno',\n",
       " 'les',\n",
       " 'ni',\n",
       " 'contra',\n",
       " 'otros',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'ante',\n",
       " 'ellos',\n",
       " 'e',\n",
       " 'esto',\n",
       " 'mí',\n",
       " 'antes',\n",
       " 'algunos',\n",
       " 'qué',\n",
       " 'unos',\n",
       " 'yo',\n",
       " 'otro',\n",
       " 'otras',\n",
       " 'otra',\n",
       " 'él',\n",
       " 'tanto',\n",
       " 'esa',\n",
       " 'estos',\n",
       " 'mucho',\n",
       " 'quienes',\n",
       " 'nada',\n",
       " 'muchos',\n",
       " 'cual',\n",
       " 'poco',\n",
       " 'ella',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'algunas',\n",
       " 'algo',\n",
       " 'nosotros',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'tú',\n",
       " 'te',\n",
       " 'ti',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'ellas',\n",
       " 'nosotras',\n",
       " 'vosotros',\n",
       " 'vosotras',\n",
       " 'os',\n",
       " 'mío',\n",
       " 'mía',\n",
       " 'míos',\n",
       " 'mías',\n",
       " 'tuyo',\n",
       " 'tuya',\n",
       " 'tuyos',\n",
       " 'tuyas',\n",
       " 'suyo',\n",
       " 'suya',\n",
       " 'suyos',\n",
       " 'suyas',\n",
       " 'nuestro',\n",
       " 'nuestra',\n",
       " 'nuestros',\n",
       " 'nuestras',\n",
       " 'vuestro',\n",
       " 'vuestra',\n",
       " 'vuestros',\n",
       " 'vuestras',\n",
       " 'esos',\n",
       " 'esas',\n",
       " 'estoy',\n",
       " 'estás',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'esté',\n",
       " 'estés',\n",
       " 'estemos',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estaré',\n",
       " 'estarás',\n",
       " 'estará',\n",
       " 'estaremos',\n",
       " 'estaréis',\n",
       " 'estarán',\n",
       " 'estaría',\n",
       " 'estarías',\n",
       " 'estaríamos',\n",
       " 'estaríais',\n",
       " 'estarían',\n",
       " 'estaba',\n",
       " 'estabas',\n",
       " 'estábamos',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estuve',\n",
       " 'estuviste',\n",
       " 'estuvo',\n",
       " 'estuvimos',\n",
       " 'estuvisteis',\n",
       " 'estuvieron',\n",
       " 'estuviera',\n",
       " 'estuvieras',\n",
       " 'estuviéramos',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuviese',\n",
       " 'estuvieses',\n",
       " 'estuviésemos',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estando',\n",
       " 'estado',\n",
       " 'estada',\n",
       " 'estados',\n",
       " 'estadas',\n",
       " 'estad',\n",
       " 'he',\n",
       " 'has',\n",
       " 'ha',\n",
       " 'hemos',\n",
       " 'habéis',\n",
       " 'han',\n",
       " 'haya',\n",
       " 'hayas',\n",
       " 'hayamos',\n",
       " 'hayáis',\n",
       " 'hayan',\n",
       " 'habré',\n",
       " 'habrás',\n",
       " 'habrá',\n",
       " 'habremos',\n",
       " 'habréis',\n",
       " 'habrán',\n",
       " 'habría',\n",
       " 'habrías',\n",
       " 'habríamos',\n",
       " 'habríais',\n",
       " 'habrían',\n",
       " 'había',\n",
       " 'habías',\n",
       " 'habíamos',\n",
       " 'habíais',\n",
       " 'habían',\n",
       " 'hube',\n",
       " 'hubiste',\n",
       " 'hubo',\n",
       " 'hubimos',\n",
       " 'hubisteis',\n",
       " 'hubieron',\n",
       " 'hubiera',\n",
       " 'hubieras',\n",
       " 'hubiéramos',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubiese',\n",
       " 'hubieses',\n",
       " 'hubiésemos',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'habiendo',\n",
       " 'habido',\n",
       " 'habida',\n",
       " 'habidos',\n",
       " 'habidas',\n",
       " 'soy',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'somos',\n",
       " 'sois',\n",
       " 'son',\n",
       " 'sea',\n",
       " 'seas',\n",
       " 'seamos',\n",
       " 'seáis',\n",
       " 'sean',\n",
       " 'seré',\n",
       " 'serás',\n",
       " 'será',\n",
       " 'seremos',\n",
       " 'seréis',\n",
       " 'serán',\n",
       " 'sería',\n",
       " 'serías',\n",
       " 'seríamos',\n",
       " 'seríais',\n",
       " 'serían',\n",
       " 'era',\n",
       " 'eras',\n",
       " 'éramos',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'fui',\n",
       " 'fuiste',\n",
       " 'fue',\n",
       " 'fuimos',\n",
       " 'fuisteis',\n",
       " 'fueron',\n",
       " 'fuera',\n",
       " 'fueras',\n",
       " 'fuéramos',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fuese',\n",
       " 'fueses',\n",
       " 'fuésemos',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'sintiendo',\n",
       " 'sentido',\n",
       " 'sentida',\n",
       " 'sentidos',\n",
       " 'sentidas',\n",
       " 'siente',\n",
       " 'sentid',\n",
       " 'tengo',\n",
       " 'tienes',\n",
       " 'tiene',\n",
       " 'tenemos',\n",
       " 'tenéis',\n",
       " 'tienen',\n",
       " 'tenga',\n",
       " 'tengas',\n",
       " 'tengamos',\n",
       " 'tengáis',\n",
       " 'tengan',\n",
       " 'tendré',\n",
       " 'tendrás',\n",
       " 'tendrá',\n",
       " 'tendremos',\n",
       " 'tendréis',\n",
       " 'tendrán',\n",
       " 'tendría',\n",
       " 'tendrías',\n",
       " 'tendríamos',\n",
       " 'tendríais',\n",
       " 'tendrían',\n",
       " 'tenía',\n",
       " 'tenías',\n",
       " 'teníamos',\n",
       " 'teníais',\n",
       " 'tenían',\n",
       " 'tuve',\n",
       " 'tuviste',\n",
       " 'tuvo',\n",
       " 'tuvimos',\n",
       " 'tuvisteis',\n",
       " 'tuvieron',\n",
       " 'tuviera',\n",
       " 'tuvieras',\n",
       " 'tuviéramos',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuviese',\n",
       " 'tuvieses',\n",
       " 'tuviésemos',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'teniendo',\n",
       " 'tenido',\n",
       " 'tenida',\n",
       " 'tenidos',\n",
       " 'tenidas',\n",
       " 'tened']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "18f0f841-b858-493f-adc7-39ee8263ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove English stopwords from the input text\n",
    "def remove_stopwords(text):\n",
    "    new_text = []  # List to store words that are NOT stopwords\n",
    "\n",
    "    # Loop through each word in the input text\n",
    "    for word in text.split():\n",
    "        # If the word is a stopword, append an empty string\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')  # This could be skipped to avoid empty strings\n",
    "        else:\n",
    "            new_text.append(word)  # Keep the non-stopword\n",
    "\n",
    "    x = new_text[:]      # Make a copy of the cleaned word list\n",
    "    new_text.clear()     # Clear the original list (optional here)\n",
    "    return \" \".join(x)   # Join the cleaned list into a string and return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c1b75c20-ffe4-4fbe-a0c4-d64ed9336f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probably  all-time favorite movie,  story  selflessness, sacrifice  dedication   noble cause,    preachy  boring.   never gets old, despite   seen   15   times'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords('probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e73f2632-22e4-4f58-a1cb-16e406b4c92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when a hapless but dedicated talent manager si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>after she betrays a powerful mob boss, a woman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>popeye is a super-strong, spinach-scarfing sai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eight strangers awaken with no memory, in a pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when a harvard-educated cia agent is killed du...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            overview\n",
       "0  when a hapless but dedicated talent manager si...\n",
       "1  after she betrays a powerful mob boss, a woman...\n",
       "2  popeye is a super-strong, spinach-scarfing sai...\n",
       "3  eight strangers awaken with no memory, in a pu...\n",
       "4  when a harvard-educated cia agent is killed du..."
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2c6111e5-4d1a-4a43-9c64-44e69a4356a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       hapless  dedicated talent manager signs  fir...\n",
       "1       betrays  powerful mob boss,  woman matches w...\n",
       "2     popeye   super-strong, spinach-scarfing sailor...\n",
       "3     eight strangers awaken   memory,   puzzling cu...\n",
       "4       harvard-educated cia agent  killed   operati...\n",
       "5     air force one  shot   terrorists, leaving  pre...\n",
       "6     sandy ricks  sent   mom  coral key,  rustic is...\n",
       "7     cindy finds   house  lives   haunted   little ...\n",
       "8     spending  summer   catskills resort   family, ...\n",
       "9      young black woman discovers   father   sperm ...\n",
       "10     ragtag group  public sector employees battle ...\n",
       "11    tom  anna wright,  young american couple, fall...\n",
       "12     art curator decides  seek revenge   abusive b...\n",
       "13    eddie, dov, yvan   others ...  warm friends  m...\n",
       "14     bright high-school senior   impending status ...\n",
       "15     14th century crusader returns   comrade   hom...\n",
       "16     prehistoric epic  follows  young mammoth hunt...\n",
       "17    ace assassin frank kitchen  double crossed  ga...\n",
       "18     scientific expedition sets   borneo  seek  fl...\n",
       "19     group  friends head   land  oaky chardonnays ...\n",
       "Name: overview, dtype: object"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overview'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f5c6f49a-7f4a-410e-a322-1d6337097995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_emoji(text):\n",
    "    # Define a regex pattern to match emojis and certain Unicode symbols\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons (smileys, faces)\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs (weather, plants, animals)\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols (vehicles, places)\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (regional indicator symbols, iOS flags)\n",
    "                               u\"\\U00002702-\\U000027B0\"  # dingbats and various symbols\n",
    "                               u\"\\U000024C2-\\U0001F251\"  # enclosed characters and other symbols\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    # Substitute all matched emojis/symbols with an empty string (i.e., remove them)\n",
    "    return emoji_pattern.sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "65450ca6-69fd-4a94-825c-51c0acb7df4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loved the movie. It was '"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"Loved the movie. It was 😘😘\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "75654e15-536b-4575-af14-00fc932b89c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lmao '"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"Lmao 😂😂\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6ba9f632-e1a0-42e3-a3f4-2bdc0303d7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "   ---------------------------------------- 0.0/590.6 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 286.7/590.6 kB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 590.6/590.6 kB 6.2 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.14.1\n",
      "Python is :fire:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\typing_extensions-4.11.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\typing_extensions-4.11.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji\n",
    "import emoji\n",
    "print(emoji.demojize('Python is 🔥'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d45ded35-4610-4945-9c13-8d3fc832a05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is :fire:\n"
     ]
    }
   ],
   "source": [
    "print(emoji.demojize('Python is 🔥'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e85f1af8-1ec0-4545-a822-c689fb4b693b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loved the movie. It was :face_blowing_a_kiss:\n"
     ]
    }
   ],
   "source": [
    "print(emoji.demojize('Loved the movie. It was 😘'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "47236353-d2c2-4f47-9a48-f1bda46aaab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenization\n",
    "sent1 = 'I am going to delhi'\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3e95954d-9550-4944-b763-ac02473f992b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to delhi',\n",
       " ' I will stay there for 3 days',\n",
       " \" Let's hope the trip to be great\"]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "sent2 = 'I am going to delhi. I will stay there for 3 days. Let\\'s hope the trip to be great'\n",
    "sent2.split('.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "9dd3c9f8-8188-4968-8a77-65cd52c39898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi!']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problems with split function\n",
    "sent3 = 'I am going to delhi!'\n",
    "sent3.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "326d9cc3-744a-441f-a5f0-08a173d389d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where do think I should go? I have 3 day holiday']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent4 = 'Where do think I should go? I have 3 day holiday'\n",
    "sent4.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3a166148-2039-4520-80b1-8a8e34aaccf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2760898009.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[190], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    2. Regular Expression\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "2. Regular Expression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e7084f29-984a-4e12-babb-a79e5e9af438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\jainj\\AppData\\Local\\Temp\\ipykernel_25296\\4158639065.py:9: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  tokens = re.findall(\"[\\w']+\", sent3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re  # Import the regular expressions module\n",
    "\n",
    "# Define a sample sentence\n",
    "sent3 = 'I am going to delhi!'\n",
    "\n",
    "# Use re.findall to extract words from the sentence\n",
    "# [\\w']+ matches one or more word characters (letters, digits, underscores) and apostrophes\n",
    "# This helps in extracting words while ignoring punctuation like '!'\n",
    "tokens = re.findall(\"[\\w']+\", sent3)\n",
    "\n",
    "# Display the list of tokens (words)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "20e82db4-ce58-494c-94d5-5b047b7511ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry',\n",
       " \"\\nLorem Ipsum has been the industry's standard dummy text ever since the 1500s, \\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sentences = re.compile('[.!?] ').split(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c11ef3-ab4d-4626-aab8-0ff1ddfca16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "606b5a01-9792-482b-add4-ea594da9581c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\typing_extensions-4.11.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\typing_extensions-4.11.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "9681d0fc-c096-453c-b53a-387ba2bb91d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'going', 'to', 'visit', 'delhi', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "sent1 = 'I am going to visit delhi!'\n",
    "tokens = tokenizer.tokenize(sent1)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "d74cc402-d2d6-43c2-bdff-d8efe4650c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 Tokens: ['Lorem', 'Ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry', '?']\n",
      "Sentence 2 Tokens: ['Lorem', 'Ipsum', 'has', 'been', 'the', 'industry', \"'s\", 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s', ',']\n",
      "Sentence 3 Tokens: ['when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen', 'book', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "\n",
    "# Naive sentence split (since Punkt is broken)\n",
    "sentences = text.split('\\n')  # or use text.split('. ') if you want dot-based splitting\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    print(f\"Sentence {i+1} Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "19c7aaec-e78f-4a1b-8b8c-5be12e34e89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']\n"
     ]
    }
   ],
   "source": [
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6 = \"We're here to help! mail us at nks@gmail.com\"\n",
    "sent7 = 'A 5km ride cost $10.50'\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sent5)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "fcc260e1-353f-4238-a2f7-6c3fc8dd88ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', \"'re\", 'here', 'to', 'help', '!', 'mail', 'us', 'at', 'nks', '@', 'gmail.com']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sent6)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "b2ae778b-cfd3-4f7e-8aa6-b80c9c5a3dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', '5km', 'ride', 'cost', '$', '10.50']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sent7)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a91549-1add-480e-bd3e-4cff799bf38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "80c59164-f542-4cb4-b948-fd55984923a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: spacy 3.8.7\n",
      "Uninstalling spacy-3.8.7:\n",
      "  Successfully uninstalled spacy-3.8.7\n",
      "Found existing installation: thinc 8.3.6\n",
      "Uninstalling thinc-8.3.6:\n",
      "  Successfully uninstalled thinc-8.3.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\~hinc'.\n",
      "You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached spacy-3.8.7-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Using cached thinc-8.3.6-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from spacy) (2.2.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\jainj\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Using cached spacy-3.8.7-cp312-cp312-win_amd64.whl (13.9 MB)\n",
      "Using cached thinc-8.3.6-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "Installing collected packages: thinc, spacy\n",
      "Successfully installed spacy-3.8.7 thinc-8.3.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\typing_extensions-4.11.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\typing_extensions-4.11.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic-2.5.3.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\pydantic_core-2.14.6.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\anaconda_cloud_auth-0.5.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\certifi-2024.6.2.dist-info due to invalid metadata entry 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\spacy\\errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\spacy\\compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\thinc\\__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\thinc\\config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\thinc\\types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\thinc\\compat.py\", line 99, in <module>\n",
      "    import h5py\n",
      "  File \"C:\\Users\\jainj\\anaconda3\\Lib\\site-packages\\h5py\\__init__.py\", line 45, in <module>\n",
      "    from ._conv import register_converters as _register_converters, \\\n",
      "  File \"h5py\\\\_conv.pyx\", line 1, in init h5py._conv\n",
      "  File \"h5py\\\\h5r.pyx\", line 1, in init h5py.h5r\n",
      "  File \"h5py\\\\h5p.pyx\", line 1, in init h5py.h5p\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[290], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install spacy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython -m spacy download en_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      5\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m doc1 \u001b[38;5;241m=\u001b[39m nlp(sent5)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\errors.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\compat.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _importlib_metadata \u001b[38;5;28;01mas\u001b[39;00m importlib_metadata  \u001b[38;5;66;03m# type: ignore[no-redef]    # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     41\u001b[0m pickle \u001b[38;5;241m=\u001b[39m pickle\n\u001b[0;32m     42\u001b[0m copy_reg \u001b[38;5;241m=\u001b[39m copy_reg\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     CupyOps,\n\u001b[0;32m      3\u001b[0m     MPSOps,\n\u001b[0;32m      4\u001b[0m     NumpyOps,\n\u001b[0;32m      5\u001b[0m     Ops,\n\u001b[0;32m      6\u001b[0m     get_current_ops,\n\u001b[0;32m      7\u001b[0m     get_ops,\n\u001b[0;32m      8\u001b[0m     set_current_ops,\n\u001b[0;32m      9\u001b[0m     set_gpu_allocator,\n\u001b[0;32m     10\u001b[0m     use_ops,\n\u001b[0;32m     11\u001b[0m     use_pytorch_for_gpu_memory,\n\u001b[0;32m     12\u001b[0m     use_tensorflow_for_gpu_memory,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_mxnet, enable_tensorflow, has_cupy\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, ConfigValidationError, registry\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\backends\\__init__.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cupy_allocators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cupy_pytorch_allocator, cupy_tensorflow_allocator\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_server\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParamServer\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcupy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CupyOps\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MPSOps\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\backends\\cupy_ops.py:16\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     is_cupy_array,\n\u001b[0;32m      8\u001b[0m     is_mxnet_gpu_array,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     torch2xp,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _custom_kernels\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@registry\u001b[39m\u001b[38;5;241m.\u001b[39mops(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCupyOps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCupyOps\u001b[39;00m(Ops):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\backends\\numpy_ops.pyx:1\u001b[0m, in \u001b[0;36minit thinc.backends.numpy_ops\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it)."
     ]
    }
   ],
   "source": [
    "!pip uninstall -y spacy thinc\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc1 = nlp(sent5)\n",
    "doc2 = nlp(sent6)\n",
    "doc3 = nlp(sent7)\n",
    "doc4 = nlp(sent1)\n",
    "for token in doc4:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "c863f9e0-9d87-474f-abdd-d3aff03109de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "1597e990-fdd1-4951-8cb9-55dd003b1f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "0d1272ca-e0be-415c-a78c-afa14a97019b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"walk walks walking walked\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "02cc91da-084d-4a8f-95d3-e3e1cbb8bcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie\n"
     ]
    }
   ],
   "source": [
    "text = 'probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie'\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "3d3c8be0-6773-4a75-8902-7f303014c67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabl my alltim favorit movi a stori of selfless sacrific and dedic to a nobl caus but it not preachi or bore it just never get old despit my have seen it some 15 or more time in the last 25 year paul luka perform bring tear to my eye and bett davi in one of her veri few truli sympathet role is a delight the kid are as grandma say more like dressedup midget than children but that onli make them more fun to watch and the mother slow awaken to what happen in the world and under her own roof is believ and startl if i had a dozen thumb theyd all be up for thi movi'"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "8ac4aaca-8a7c-4be2-89bc-58c94919977e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jainj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jainj\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\jainj/nltk_data'\n    - 'C:\\\\Users\\\\jainj\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\jainj\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\jainj\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\jainj\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:/Users/jainj/nltk_data'\n    - 'C:/Users/jainj/nltk_data'\n    - 'C:/Users/jainj/AppData/Roaming/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[304], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHe was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m punctuations \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?:!.,;\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m sentence_words \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(sentence)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Removing punctuation tokens - better to use list comprehension to avoid modifying list while iterating\u001b[39;00m\n\u001b[0;32m     16\u001b[0m sentence_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence_words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m punctuations]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\jainj/nltk_data'\n    - 'C:\\\\Users\\\\jainj\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\jainj\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\jainj\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\jainj\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:/Users/jainj/nltk_data'\n    - 'C:/Users/jainj/nltk_data'\n    - 'C:/Users/jainj/AppData/Roaming/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required nltk data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations = \"?:!.,;\"\n",
    "\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Removing punctuation tokens - better to use list comprehension to avoid modifying list while iterating\n",
    "sentence_words = [word for word in sentence_words if word not in punctuations]\n",
    "\n",
    "print(\"{0:20}{1:20}\".format(\"Word\", \"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print(\"{0:20}{1:20}\".format(word, wordnet_lemmatizer.lemmatize(word, pos='v')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b652a599-c51c-4c4a-86ba-bdf47212f185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
